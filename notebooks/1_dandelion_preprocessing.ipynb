{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *dandelion* Notebook-1\n",
    "\n",
    "![dandelion_logo](img/dandelion_logo.png)\n",
    "## Foreword\n",
    "***dandelion*** is written in `python==3.7.6` and it is primarily a single-cell BCR-seq analysis package. It makes use of some tools from the fantastic [*immcantation suite*](https://immcantation.readthedocs.io/) and the main idea is that it implements a workflow for the pre-processing and exploratory stages with integrated use of tools from *immcantation* for the BCR side of things and analysis tools from [*scanpy*](https://scanpy.readthedocs.io/) for the RNA-seq side of things. I hope to be able to introduce some new single-cell BCR-seq exploratory tools down the road through *dandelion*. \n",
    "\n",
    "\n",
    "## Pre-processing\n",
    "This notebook will cover the initial pre-processing of files after 10X's `cellranger vdj` immune profiling data analysis pipeline. The directory structure of a 10x output folder will typically look like this:\n",
    "```console\n",
    "(dandelion) mib113557i:Pan_Immune_BCR kt16$ tree Pan_T7918901\n",
    "Pan_T7918901\n",
    "├── all_contig.fasta\n",
    "├── all_contig.fasta.fai\n",
    "├── clonotypes.csv\n",
    "├── concat_ref.fasta\n",
    "├── concat_ref.fasta.fai\n",
    "├── consensus.fasta\n",
    "├── consensus.fasta.fai\n",
    "├── consensus_annotations.csv\n",
    "├── filtered_contig.fasta\n",
    "├── filtered_contig_annotations.csv\n",
    "└── metrics_summary.csv\n",
    "```\n",
    "\n",
    "At this stage, ***dandelion*** only needs the fasta files to start, particularly either *all_contig.fasta* or *filtered_contig.fasta*.\n",
    "\n",
    "In this notebook, I'm running everything with the *all_contig.fasta* files to get a sense of how long it would take when the files are considerably larger. I'm using a standard laptop for the analysis here: entry level 2017 Macbook Pro with 2.3 GHz Intel Core i5 processor and 16 GB 2133 MHz LPDDR3 ram.\n",
    "\n",
    "#### Before starting, a couple of environmental variables need to be set up to make it run smoothly:\n",
    "\n",
    "In ***shell***, export the path to the database folders like as follows:\n",
    "```bash\n",
    "echo \"export GERMLINE=/Users/kt16/Documents/Github/dandelion/database/germlines/\" >> ~/.bash_profile\n",
    "echo \"export IGDATA=/Users/kt16/Documents/Github/dandelion/database/igblast/\" >> ~/.bash_profile\n",
    "echo \"export BLASTDB=/Users/kt16/Documents/Github/dandelion/database/blast/\" >> ~/.bash_profile\n",
    "echo \"export PATH=/Users/kt16/Documents/Github/dandelion/bin:$PATH\" >> ~/.bash_profile\n",
    "source ~/.bash_profile\n",
    "```\n",
    "The databases for igblast are basically setup using [changeo's instructions](https://changeo.readthedocs.io/en/stable/examples/igblast.html). The instruction for setting up blast database is simpler and will be covered later in this notebook.\n",
    "\n",
    "Also check if the softwares can be found:\n",
    "```console\n",
    "(mypython3) mib113557i:~ kt16$ conda activate dandelion\n",
    "(dandelion) mib113557i:~ kt16$ which makeblastdb\n",
    "/Users/kt16/Documents/Github/dandelion/bin/makeblastdb\n",
    "(dandelion) mib113557i:~ kt16$ which blastn\n",
    "/Users/kt16/Documents/Github/dandelion/bin/blastn\n",
    "(dandelion) mib113557i:~ kt16$ which igblastn\n",
    "/Users/kt16/Documents/Github/dandelion/bin/igblastn\n",
    "(dandelion) mib113557i:~ kt16$ which tigger-genotype.R\n",
    "/Users/kt16/Documents/Github/dandelion/bin/tigger-genotype.R\n",
    "```\n",
    "\n",
    "If you don't have the softwares, download [blast+](https://ftp.ncbi.nih.gov/blast/executables/igblast/release/LATEST/) and [igblast](https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/). For tigger-genotype, you can download it [here](https://bitbucket.org/kleinstein/immcantation/src/default/pipelines/). Just note that I made some minor modifications to this file, hence there's version specific to this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kt16/miniconda3/envs/dandelion/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import RangeIndex\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import os\n",
    "os.chdir(os.path.expanduser('/Users/kt16/Documents/Github/dandelion'))\n",
    "import dandelion as ddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kt16/Documents/Clatworthy_scRNAseq/Ondrej/PIP/Pan_Immune_BCR'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory to somewhere more workable\n",
    "os.chdir(os.path.expanduser('/Users/kt16/Documents/Clatworthy_scRNAseq/Ondrej/PIP/Pan_Immune_BCR/'))\n",
    "# print current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "#### Formatting the headers of the cellranger fasta file\n",
    "This step immediately below is optional and is just a lazy way to make a dictionary from an external file using a utility function `utl.dict_from_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dictionary from a meta data file.\n",
    "sampledict = ddl.utl.dict_from_table('/Users/kt16/Documents/Clatworthy_scRNAseq/Ondrej/dandelion_files/meta/PIP_sampleInfo_kt16.txt', columns = ('SANGER SAMPLE ID', 'GEX_SAMPLE_ID')) # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding a sample prefix to the headers of each contig in the fasta files using the dictionary created above, via the function `pp.format_fasta(s)`. The prefix is basically just the folder name, so in this case it's `Pan_T7918901`. The function will also create subfolders where the new fasta file and all subsequent files will be located. The file structure should look something like this later on if the settings are left as default.\n",
    "```console\n",
    "(dandelion) mib113557i:Pan_Immune_BCR kt16$ tree Pan_T7918901\n",
    "Pan_T7918901\n",
    "├── all_contig.fasta\n",
    "├── all_contig.fasta.fai\n",
    "├── clonotypes.csv\n",
    "├── concat_ref.fasta\n",
    "├── concat_ref.fasta.fai\n",
    "├── consensus.fasta\n",
    "├── consensus.fasta.fai\n",
    "├── consensus_annotations.csv\n",
    "├── dandelion\n",
    "│   └── data\n",
    "│       ├── all_contig.fasta\n",
    "│       ├── all_contig_igblast.tsv\n",
    "│       ├── all_contig_igblast_db-pass.tsv\n",
    "│       └── tmp\n",
    "│           ├── all_contig_igblast.blastsummary.txt\n",
    "│           ├── all_contig_igblast.fmt7\n",
    "│           └── all_contig_igblast.xml\n",
    "├── filtered_contig.fasta\n",
    "├── filtered_contig_annotations.csv\n",
    "└── metrics_summary.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formating fasta(s) : 100%|██████████| 13/13 [00:37<00:00,  2.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# the first option is a list of fasta files to format and the second option is the prefix to add to each file.\n",
    "samples = ['Pan_T7918901', 'Pan_T7918902', 'Pan_T7918903', 'Pan_T7918904', 'Pan_T7918905', 'Pan_T7918906', 'Pan_T7918907', 'Pan_T7918908', 'Pan_T7918909', 'Pan_T7918910', 'Pan_T7918912', 'Pan_T7918913', 'Pan_T7918914']\n",
    "ddl.pp.format_fastas([str(s)+'/all_contig.fasta' for s in samples], [sampledict[s] for s in samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is just a wrapper for a for-loop:\n",
    "```python\n",
    "for s in samples:\n",
    "    filePath = s+'/all_contig.fasta'\n",
    "    ddl.pp.format_fasta(filePath, sampledict[s])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "#### Reannotate the V/D/J genes with *igblastn*.\n",
    "\n",
    "`pp.reannotate_genes` uses [*changeo*](https://changeo.readthedocs.io/en/stable/examples/10x.html)'s scripts to call *igblastn* to reannotate the fasta files. Depending on the file format option, it will parse out as either an `airr` (default) or `changeo`-legacy TSV file. Importantly, with the recent update to changeo v1.0.0, all the column headers are now adhereing to the [*AIRR*](http://docs.airr-community.org/) standard (lowercase and some column name changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning genes : 100%|██████████| 13/13 [27:44<00:00, 128.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# reannotate the vdj genes with igblastn and parses output to 'airr' (default) or 'changeo' tsv formats using changeo v1.0.0 scripts\n",
    "ddl.pp.reannotate_genes(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the suffixes at the end of the files are different for the different output format, the output files shouldn't overwrite each other.\n",
    "\n",
    "But anyway, either format should work for subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning genes : 100%|██████████| 13/13 [27:51<00:00, 128.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# to write as changeo format\n",
    "ddl.pp.reannotate_genes(samples, fileformat = 'changeo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "#### Assigning constant region calls\n",
    "\n",
    "10x's \\*annotation.csv file provides a *c_gene* column, but rather than simply relying on 10x's annotation, [hk6](https://twitter.com/hamish_king) reccomended using [*immcantation-presto*'s *MaskPrimers.py*](https://presto.readthedocs.io/en/version-0.5.3---license-change/tools/MaskPrimers.html) with his custom primer list and I tested that; worked well but it took ***20 min*** for the first file (~6k contigs). It also only calls the constant region for the heavy chains. \n",
    "\n",
    "To try and speed that up, I wrote a pre-processing function, `pp.assign_isotype`, to use *blast* to annotate constant region calls for all contigs and retrieves the call, merging it with the tsv files.\n",
    "\n",
    "This function will simply overwrite the output from previous steps and adds a *c_call* column at the end.\n",
    "\n",
    "==========================\n",
    "\n",
    "Before running, the there is a need to set up a database with IMGT constant gene fasta sequences using *makeblastdb*,\n",
    "basically following the instructions from https://www.ncbi.nlm.nih.gov/books/NBK279688/.\n",
    "\n",
    "The fasta files were downloaded from IMGT and only sequences corresponding to *CH1* region for each constant gene/allele were retained. The headers were trimmed to only keep the gene and allele information. Links to find the sequences can be found here : [***human***](http://www.imgt.org/genedb/GENElect?query=7.2+IGHC&species=Homo+sapiens) and [***mouse***](http://www.imgt.org/genedb/GENElect?query=7.2+IGHC&species=Mus).\n",
    "\n",
    "I've written a utility function `utl.makeblastdb` to prep the fasta files/databases prior to running.\n",
    "```python\n",
    "ddl.utl.makeblastdb('/Users/kt16/Documents/Github/dandelion/database/blast/human/human_BCR_C.fasta')\n",
    "```\n",
    "\n",
    "We really only need to do it once and then the file path can be added as an environmental variable (like above). I've set it up that we only need to point to the blast folder and ***dandelion*** will have options to specify which organisms to use in specific functions.\n",
    "\n",
    "```bash\n",
    "echo \"export BLASTDB=/Users/kt16/Documents/Github/dandelion/database/blast/\" >> ~/.bash_profile\n",
    "source ~/.bash_profile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 6356/6356 [02:06<00:00, 50.23it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 502/502 [00:00<00:00, 1395.87it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 3835/3835 [00:41<00:00, 92.69it/s] \n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 8651/8651 [02:58<00:00, 48.53it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 10213/10213 [04:15<00:00, 40.03it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 32052/32052 [38:51<00:00, 13.75it/s] \n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 10880/10880 [04:54<00:00, 36.93it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 3374/3374 [00:29<00:00, 115.75it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 4656/4656 [01:08<00:00, 67.63it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 5352/5352 [01:22<00:00, 64.83it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 10922/10922 [03:25<00:00, 53.12it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 16829/16829 [11:58<00:00, 23.42it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 18745/18745 [10:09<00:00, 30.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for s in samples:\n",
    "    filePath = s+'/dandelion/data/all_contig.fasta'\n",
    "    ddl.pp.assign_isotype(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial assessment by just comparing what isn't matching between this method and *MaskPrimers.py* was basically the light chains constant regions were not called by *MarkPrimers.py* (because lack of input IgK/L primer sequences). But as far as i could see, it caught most of the discrepancies that [hk6](https://twitter.com/hamish_king) was talking about (IGHA1/IGHA2 and IGHG2/IGHG4 mis-calling) and also reduced the processing time by 10x (***~20 min*** to ***~2 min***), so I'm happy with this.\n",
    "\n",
    "This still takes a while when dealing with large files; the number of cpus to size of file isn't exactly linear, but I enabled parallelization as default because there were noticeable improvements in processing speeds with the smaller files. Maybe it will work better on a cluster with more cpus, rather than just a standard laptop. Other than 1 sample that took about ***~40min***, most ran within ***2-5min***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the function can be run with\n",
    "```python\n",
    "fileformat = 'changeo'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 6356/6356 [01:37<00:00, 65.22it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 502/502 [00:00<00:00, 1485.45it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 3835/3835 [00:32<00:00, 117.81it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 8651/8651 [02:27<00:00, 58.85it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 10213/10213 [03:41<00:00, 46.15it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 32052/32052 [34:28<00:00, 15.50it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 10880/10880 [04:48<00:00, 37.71it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 3374/3374 [00:28<00:00, 117.19it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 4656/4656 [01:08<00:00, 67.88it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 5352/5352 [01:22<00:00, 64.78it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 10922/10922 [03:28<00:00, 52.50it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 16829/16829 [12:11<00:00, 22.99it/s]\n",
      "Retrieving contant region calls, parallelizing with 4 cpus : 100%|██████████| 18745/18745 [10:26<00:00, 29.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for s in samples:\n",
    "    filePath = s+'/dandelion/data/all_contig.fasta'\n",
    "    ddl.pp.assign_isotype(filePath, fileformat = 'changeo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 *(optional)*:\n",
    "#### Reassigning heavy chain V gene alleles.\n",
    "\n",
    "Last step for part one of pre-processing is to use *immcantation-tigger*'s method to reassign allelic calls with `pp.reassign_alleles`. \n",
    "\n",
    "This impact's on how ***dandelion*** picks contigs to go forward for finding clones so it's highly reccomended to run it. It's also important when condering to do mutational analysis. However, the main caveat is that this needs to be run on multiple samples from the same subject, allowing for more information to be used to confidently assign a genotype *v_call*. In this case, all the samples I was processing have been from different organs from a single patient. So while important, this step can be skipped if you don't have the samples, or the patience, to do this. Having said that, this step works pretty quickly. \n",
    "\n",
    "Unfortunately, I don't have any idea whether the same method will work on light chains. Currently it just runs everything, and assigns a genotyped heavy chain V call. The light chains V calls are then transferred back from the original calls from *igblastn*. It should be technically feasible to run it through with a light chain option but I will leave it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data file(s) : 100%|██████████| 13/13 [00:01<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating objects\n",
      "   Writing out concatenated object\n",
      "      Reassigning alleles\n",
      "   Reading genotyped object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Returning light chain V calls: 100%|██████████| 36313/36313 [00:08<00:00, 4096.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving corrected genotyped object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing out to individual folders : 100%|██████████| 13/13 [00:02<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# this is also a for loop for multiple samples from the same subject\n",
    "ddl.pp.reassign_alleles(samples, out_folder = 'A31', sample_dict = sampledict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data file(s) : 100%|██████████| 13/13 [00:00<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating objects\n",
      "   Writing out concatenated object\n",
      "      Reassigning alleles\n",
      "   Reading genotyped object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Returning light chain V calls: 100%|██████████| 45213/45213 [00:10<00:00, 4121.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saving corrected genotyped object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing out to individual folders : 100%|██████████| 13/13 [00:01<00:00,  8.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# doing the same thing for 'changeo' format\n",
    "ddl.pp.reassign_alleles(samples, out_folder = 'A31', fileformat = 'changeo', sample_dict = sampledict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dandelion)",
   "language": "python",
   "name": "dandelion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
